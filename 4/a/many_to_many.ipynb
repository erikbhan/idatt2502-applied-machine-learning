{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_encodings = [\n",
    "    [1., 0., 0., 0., 0., 0., 0., 0.],  # ' '\n",
    "    [0., 1., 0., 0., 0., 0., 0., 0.],  # 'h'\n",
    "    [0., 0., 1., 0., 0., 0., 0., 0.],  # 'e'\n",
    "    [0., 0., 0., 1., 0., 0., 0., 0.],  # 'l'\n",
    "    [0., 0., 0., 0., 1., 0., 0., 0.],  # 'o'\n",
    "    [0., 0., 0., 0., 0., 1., 0., 0.],  # 'w'\n",
    "    [0., 0., 0., 0., 0., 0., 1., 0.],  # 'r'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 1.],  # 'd'\n",
    "]\n",
    "encoding_size = len(char_encodings)\n",
    "\n",
    "index_to_char = [' ', 'h', 'e', 'l', 'o', 'w', 'r', 'd']\n",
    "\n",
    "x_train = torch.tensor([[char_encodings[0]], [char_encodings[1]], [char_encodings[2]], [char_encodings[3]], [char_encodings[3]],\n",
    "                        [char_encodings[4]], [char_encodings[0]], [char_encodings[5]], [char_encodings[4]], [char_encodings[6]], [char_encodings[3]], [char_encodings[7]], [char_encodings[0]]])  # ' hello world '\n",
    "y_train = torch.tensor([char_encodings[0],char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[3],\n",
    "                        char_encodings[4], char_encodings[0], char_encodings[5], char_encodings[4], char_encodings[6], char_encodings[3], char_encodings[7], char_encodings[0]])  # ' hello world '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongShortTermMemoryModel(nn.Module):\n",
    "    def __init__(self, encoding_size):\n",
    "        super(LongShortTermMemoryModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(encoding_size, 128)  # 128 is the state size\n",
    "        self.dense = nn.Linear(128, encoding_size)  # 128 is the state size\n",
    "\n",
    "    def reset(self):  # Reset states prior to new input sequence\n",
    "        zero_state = torch.zeros(1, 1, 128)  # Shape: (number of layers, batch size, state size)\n",
    "        self.hidden_state = zero_state\n",
    "        self.cell_state = zero_state\n",
    "\n",
    "    def logits(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        out, (self.hidden_state, self.cell_state) = self.lstm(x, (self.hidden_state, self.cell_state))\n",
    "        return self.dense(out.reshape(-1, 128))\n",
    "\n",
    "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        return torch.softmax(self.logits(x), dim=1)\n",
    "\n",
    "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\n",
    "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lllll                                              \n",
      " hllll  wrrld                                       \n",
      " helll  world    rll    rlld    lld    rld    rll   \n",
      "hhelll  world   rlld   rlld   rlld   rlld   rlld   r\n",
      "hhello  world   rlld   rlld   rlld   rlld   rlld   r\n",
      "hhello wwrrld   rlld   rlld   rll   orld   rrld   rr\n",
      " hello worrld   rll   orld   rrld   rlld   rlld   rl\n",
      " hello worlld   rll   orld   rlld   rlld  orld   rrl\n",
      " hello worlld  rrld   rlld   rlld  orld   rrld   rll\n",
      " hello worll   rrld   rlld   rll   orld   rlld   rll\n",
      " hello world   hrld   rlld   rll   orld   rlld  orld\n",
      " hello world   hell  wrlld   rll   orld   rlld  orld\n",
      " hello world   hlld   rlld  hrll   rrld   rlld  orld\n",
      " hello world   hlld   rlld  hrld   rlld   rll   rrld\n",
      " hello world   hlld   rlld  hrld   rlld   rll   rrld\n",
      " hello world   hlld   rlld  hrld   rlld   rll   rrld\n",
      " hello world   hlld  orld   hlld   rlld  hrld   rlld\n",
      " hello world   hlld  orld   hlld   rlld  hrld   rlld\n",
      " hello world   hlld  orld   hlld   rll   hrld   rlld\n",
      " hello world   hlld  orld   hlld  orld   hlld   rlld\n",
      " hello world   hlld  orld   hlld  orld   hlld  orld \n",
      " hello world   hlld  orld   hlld  orld   hlld  orld \n",
      " hello world   hlld  orld   hlld  orld   hlld  orld \n",
      " hello world   hlld  orld   hlld  orld   hlld  orld \n",
      " hello world   hlld  orld   hlld  orld   hlld  orld \n",
      " hello world   hlld world   rlld world  hhlld world \n",
      " hello world   hlld world  hhlld world  hhlld world \n",
      " hello world   hlld world  hhlld world  hhlld world \n",
      " hello world   hlld world  hhlld world  hhlld world \n",
      " hello world  hhlld world  hhlld world  hhlld world \n",
      " hello world  hhlld world  hhlld world  hhlld world \n",
      " hello world  hhlld world  hhlld world  hhlld world \n",
      " hello world  hhlld world  hhlld world  hhlld world \n",
      " hello world  hhlld world  hhlld world  hhlld world \n",
      " hello world  hhlld world  hhlld world  hhlld world \n",
      " hello world  hhlld world  hhlld world  hhlld world \n",
      " hello world  hhlld world  hhlld world  hhlld world \n",
      " hello world  hhlld world  hhlld world  hhlld world \n",
      " hello world  hhlld world  hhlld world  hhlld world \n",
      " hello world  hhlld world  hhlld world  helld world \n",
      " hello world  hhlld world  helld world  helld world \n",
      " hello world  hhlld world  helld world  helld world \n",
      " hello world  hhlld world  helld world  helld world \n",
      " hello world  hhlld world  helld world  helld world \n",
      " hello world  hhlld world  helld world  helld world \n",
      " hello world  hhlld world  helld world  helld world \n",
      " hello world  hhlld world  helld world  helld world \n",
      " hello world  hhlld world  helld world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  helld world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n",
      " hello world  hello world  hello world  hello world \n"
     ]
    }
   ],
   "source": [
    "model = LongShortTermMemoryModel(encoding_size)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), 0.001)\n",
    "for epoch in range(1000):\n",
    "    model.reset()\n",
    "    model.loss(x_train, y_train).backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        # Generate characters from the initial characters ' h'\n",
    "        model.reset()\n",
    "        text = ''\n",
    "        y = model.f(torch.tensor([[char_encodings[0]]]))\n",
    "        text += index_to_char[y.argmax(1)]\n",
    "        y = model.f(torch.tensor([[char_encodings[1]]]))\n",
    "        text += index_to_char[y.argmax(1)]\n",
    "        for c in range(50):\n",
    "            y = model.f(torch.tensor([[char_encodings[y.argmax(1)]]]))\n",
    "            text += index_to_char[y.argmax(1)]\n",
    "        print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56dfad77c500f65520d1945ae902eb7eea5df12190d360e5f61446e91c077ffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
